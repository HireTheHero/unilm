config:
  batch_size: 3072
  data_root: /home/s2140401/data/coco/arrows
  datasets:
  - coco
  decay_power: 1
  draw_false_image: 0
  draw_false_text: 0
  drop_path_rate: 0.2
  end_lr: 0
  exp_name: finetune_irtr_coco_base_image384
  fast_dev_run: false
  get_recall_metric: true
  get_recall_rerank_metric: false
  image_only: false
  image_size: 384
  k_test: 32
  learning_rate: 3.0e-05
  load_path: /home/s2140401/models/vlmo/vlmo_base_patch16_384_coco.pt
  log_dir: result
  loss_names:
    irtr: 1.0
    itc: 0
    itm: 0
    mlm: 0
    nlvr2: 0
    textmlm: 0
    vqa: 0
  lr_mult: 1
  max_epoch: 50
  max_steps: 3000
  max_text_len: 40
  max_text_len_of_initckpt: 196
  mlm_prob: 0.15
  model_arch: vlmo_base_patch16
  num_gpus: 1
  num_nodes: 1
  num_workers: 8
  optim_type: adamw
  per_gpu_batchsize: 8
  precision: 16
  resume_during_training: false
  resume_from: null
  seed: 1
  test_only: true
  text_only: false
  tokenizer: bert-base-uncased
  train_transform_keys:
  - square_transform_randaug
  use_sharded_training: false
  val_check_interval: 1.0
  val_transform_keys:
  - square_transform
  vocab_size: 30522
  vqav2_label_size: 3129
  warmup_steps: 300
  weight_decay: 0.01
  whole_word_masking: false
